{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from theano.tensor.signal import downsample\n",
    "from theano.tensor.nnet import conv\n",
    "from logistic_sgd2 import LogisticRegression, load_data\n",
    "from mlp import HiddenLayer\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "path = '/home/dl-box/Liver' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class LeNetConvPoolLayer(object):\n",
    "    \"\"\"畳み込みニューラルネットの畳み込み層＋プーリング層\"\"\"\n",
    "    def __init__(self, rng, input, image_shape, filter_shape, poolsize=(2, 2)):\n",
    "        # 入力の特徴マップ数は一致する必要がある\n",
    "        assert image_shape[1] == filter_shape[1]\n",
    "\n",
    "        fan_in = np.prod(filter_shape[1:])\n",
    "        fan_out = filter_shape[0] * np.prod(filter_shape[2:]) / np.prod(poolsize)\n",
    "\n",
    "        W_bound = np.sqrt(6.0 / (fan_in + fan_out))#重み付け層の前後の隠れ層の数　重みの初期値を定義ー論文:Glorot & Bengio (2010)\n",
    "        self.W = theano.shared(\n",
    "            np.asarray(rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "                       dtype=theano.config.floatX),  # @UndefinedVariable\n",
    "            borrow=True)\n",
    "\n",
    "        b_values = np.zeros((filter_shape[0],), dtype=theano.config.floatX)  # @UndefinedVariable\n",
    "        self.b = theano.shared(value=b_values, borrow=T)#フィルター=W，バイアス=b\n",
    "\n",
    "        # 入力の特徴マップとフィルタの畳み込み\n",
    "        conv_out = conv.conv2d(\n",
    "            input=input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            image_shape=image_shape)#filter_shape:カーネルサイズ,image_filter:最初のサイズ\n",
    "\n",
    "        # Max-poolingを用いて各特徴マップをダウンサンプリング\n",
    "        pooled_out = downsample.max_pool_2d(\n",
    "            input=conv_out,\n",
    "            ds=poolsize,\n",
    "            ignore_border=True)\n",
    "\n",
    "        # バイアスを加える\n",
    "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))#ここではtanh()で処理している\n",
    "\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "\n",
    "def evaluate_lenet5(learning_rate=0.0005, n_epochs=1,\n",
    "                            dataset=\"/home/dl-box/Liver\", batch_size=1000):\n",
    "    rng = np.random.RandomState(23455)\n",
    "\n",
    "\n",
    "            # 学習データのロード\n",
    "            #datasets = load_data(dataset)\n",
    "    with open('/home/dl-box/Liver/sotsuron_train/train_multi_PV_black.pkl','rb') as f1:\n",
    "    #with open('/home/dl-box/Liver/Cyst_data1016/train_dataset1019/3phase_1220.pkl','rb') as f1:\n",
    "                datasets1 = pickle.load(f1)\n",
    "            \n",
    "    with open('/home/dl-box/Liver/sotsuron_data/test_data/META506_NC.pkl','rb') as f2:\n",
    "    #with open('/home/dl-box/Liver/Cyst_data1016/test_dataset1019/test_106.pkl','rb') as f2:\n",
    "                datasets2 = pickle.load(f2)\n",
    "    \n",
    "    train_set_x, train_set_y = datasets1[0]\n",
    "    valid_set_x, valid_set_y = datasets1[1]\n",
    "    test_set_x, test_set_y = datasets2[0]\n",
    "    \n",
    "    train_set_x = np.array(train_set_x)\n",
    "    train_set_y = np.array(train_set_y)    \n",
    "    valid_set_x = np.array(valid_set_x)\n",
    "    valid_set_y = np.array(valid_set_y)  \n",
    "    test_set_x = np.array(test_set_x)\n",
    "    test_set_y = np.array(test_set_y)  \n",
    "    \n",
    "    print train_set_x.shape\n",
    "    \n",
    "    train_set_x = train_set_x.reshape(train_set_x.shape[0],-1)\n",
    "    train_set_y = train_set_y.reshape(train_set_y.shape[0],-1)\n",
    "    valid_set_x = valid_set_x.reshape(valid_set_x.shape[0],-1)\n",
    "    valid_set_y = valid_set_y.reshape(valid_set_y.shape[0],-1)\n",
    "    test_set_x = test_set_x.reshape(test_set_x.shape[0],-1)\n",
    "    test_set_y = test_set_y.reshape(test_set_y.shape[0],-1)\n",
    "    \n",
    "    print train_set_x.shape\n",
    "    \n",
    "    train_dimension = train_set_x.shape[0]\n",
    "    valid_dimension = valid_set_x.shape[0]\n",
    "    test_dimension = test_set_x.shape[0]\n",
    "    \n",
    "    print train_dimension\n",
    "    print test_dimension\n",
    "\n",
    "    n_train_batches = train_dimension / batch_size\n",
    " \n",
    "    n_valid_batches = valid_dimension / batch_size\n",
    "    \n",
    "    n_test_batches = test_dimension / batch_size\n",
    "    \n",
    "    print n_train_batches\n",
    "    print n_test_batches\n",
    "\n",
    "   \n",
    "    train_set_x = theano.shared(np.asarray(train_set_x,dtype=theano.config.floatX),borrow=True)\n",
    "    train_set_y = theano.shared(np.asarray(train_set_y,dtype=theano.config.floatX),borrow=True)   \n",
    "    valid_set_x = theano.shared(np.asarray(valid_set_x,dtype=theano.config.floatX),borrow=True)\n",
    "    valid_set_y = theano.shared(np.asarray(valid_set_y,dtype=theano.config.floatX),borrow=True)\n",
    "    test_set_x = theano.shared(np.asarray(test_set_x,dtype=theano.config.floatX),borrow=True)\n",
    "    test_set_y = theano.shared(np.asarray(test_set_y,dtype=theano.config.floatX),borrow=True)\n",
    "    \n",
    "            # ミニバッチのインデックスを表すシンボル\n",
    "    \n",
    "    index = T.lscalar() #←change\n",
    " \n",
    "            # ミニバッチの学習データ(ピクセル)とラベル(0－9)を表すシンボル\n",
    "    \n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y')\n",
    "    \n",
    "    # 入力\n",
    "    # 入力のサイズを4Dテンソルに変換\n",
    "    # batch_sizeは訓練画像の枚数\n",
    "    # チャンネル数は1\n",
    "    # (28, 28)はMNISTの画像サイズ\n",
    "    layer0_input = x.reshape((batch_size, 1, 29, 29))   #←エラー起きてる？\n",
    " \n",
    "    # 最初の畳み込み層+プーリング層\n",
    "    # 畳み込みに使用するフィルタサイズは5x5ピクセル\n",
    "    # 畳み込みによって画像サイズは28x28ピクセルから24x24ピクセルに落ちる\n",
    "    # プーリングによって画像サイズはさらに12x12ピクセルに落ちる\n",
    "    # 特徴マップ数は20枚でそれぞれの特徴マップのサイズは12x12ピクセル\n",
    "    # 最終的にこの層の出力のサイズは (batch_size, 20, 12, 12) になる\n",
    "   \n",
    "    layer0 = LeNetConvPoolLayer(rng,\n",
    "                input=layer0_input,\n",
    "                image_shape=(batch_size, 1, 29, 29),  # 入力画像のサイズを4Dテンソルで指定\n",
    "                filter_shape=(20, 1, 6, 6),           # フィルタのサイズを4Dテンソルで指定\n",
    "                poolsize=(2, 2))\n",
    "    \n",
    "    #print layer0_input\n",
    "    # layer0の出力がlayer1への入力となる\n",
    "    # layer0の出力画像のサイズは (batch_size, 20, 12, 12)\n",
    "    # 12x12ピクセルの画像が特徴マップ数分（20枚）ある\n",
    "    # 畳み込みによって画像サイズは12x12ピクセルから8x8ピクセルに落ちる\n",
    "    # プーリングによって画像サイズはさらに4x4ピクセルに落ちる\n",
    "    # 特徴マップ数は50枚でそれぞれの特徴マップのサイズは4x4ピクセル\n",
    "    # 最終的にこの層の出力のサイズは (batch_size, 50, 4, 4) になる\n",
    "   \n",
    "    layer1 = LeNetConvPoolLayer(rng,\n",
    "                input=layer0.output,\n",
    "                image_shape=(batch_size, 20, 12, 12), # 入力画像のサイズを4Dテンソルで指定\n",
    "                filter_shape=(50, 20, 5, 5),          # フィルタのサイズを4Dテンソルで指定\n",
    "                poolsize=(2, 2))\n",
    "\n",
    "    # 隠れ層への入力\n",
    "    # 画像のピクセルをフラット化する\n",
    "    # layer1の出力のサイズは (batch_size, 50, 4, 4) なのでflatten()によって\n",
    "    # (batch_size, 50*4*4) = (batch_size, 800) になる\n",
    "    layer2_input = layer1.output.flatten(2)\n",
    "\n",
    "    # 全結合された隠れ層\n",
    "    # 入力が800ユニット、出力が500ユニット\n",
    "  \n",
    "    layer2 = HiddenLayer(rng,\n",
    "        input=layer2_input,\n",
    "        n_in=50 * 4 * 4,\n",
    "        n_out=300,\n",
    "        activation=T.tanh)\n",
    "\n",
    "    # 最終的な数字分類を行うsoftmax層\n",
    "    layer3 = LogisticRegression(input=layer2.output, n_in=300, n_out=2)\n",
    "\n",
    "  \n",
    "    # コスト関数を計算するシンボル\n",
    "    cost = layer3.negative_log_likelihood(y)\n",
    "\n",
    "    #test_x = T.lvector(test_set_x)\n",
    "    #print(test_x[0:100])\n",
    "    #test_y = T.lvector(test_set_y)\n",
    "    #test_set_x = test_set_x.flatten()\n",
    "    \n",
    "    test_set_y = test_set_y.flatten()#change from matrix to vector \n",
    "    \n",
    "  \n",
    "    \n",
    "    #test_x = T.cast(test_x,'int32')\n",
    "    \n",
    "    test_set_y = T.cast(test_set_y,'int32')#cast from float32 to int32\n",
    " \n",
    "    \n",
    "    value = layer3.makefile1(y)\n",
    "    \n",
    "    \n",
    "    # index番目のテスト用ミニバッチを入力してエラー率を返す関数を定義\n",
    "    test_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        })\n",
    "    \n",
    "    value_model = theano.function(\n",
    "        [index],\n",
    "        value,\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        })\n",
    "\n",
    " \n",
    "    valid_set_y = valid_set_y.flatten()#change from matrix to vector \n",
    "    \n",
    "\n",
    "    valid_set_y = T.cast(valid_set_y,'int32')#cast from float32 to int32\n",
    "  \n",
    "    # index番目のバリデーション用ミニバッチを入力してエラー率を返す関数を定義\n",
    "    \n",
    "    \n",
    "    validate_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        })\n",
    "\n",
    "    # パラメータ\n",
    "    params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "    #print(params)\n",
    "\n",
    "    # コスト関数の微分\n",
    "    grads = T.grad(cost, params)\n",
    "\n",
    "    # パラメータ更新式\n",
    "    updates = [(param_i, param_i - learning_rate * grad_i) for param_i, grad_i in zip(params, grads)]\n",
    "    #print(updates)\n",
    "    \n",
    "   \n",
    "    train_set_y = train_set_y.flatten()#change from matrix to vector \n",
    "    \n",
    "\n",
    "    train_set_y = T.cast(train_set_y,'int32')#cast from float32 to int32\n",
    " \n",
    "    # index番目の訓練バッチを入力し、パラメータを更新する関数を定義\n",
    "    \n",
    "    train_model = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        })\n",
    "\n",
    "\n",
    "    # eary-stoppingのパラメータ\n",
    "    patience = 20000\n",
    "    patience_increase = 2\n",
    "    improvement_threshold = 0.95\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "  \n",
    "\n",
    "    best_validation_loss = np.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0\n",
    "    start_time = time.clock()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "\n",
    "    fp1 = open(\"validation_error.txt\", \"w\")\n",
    "    fp2 = open(\"test_error.txt\", \"w\")\n",
    "    f = open(path+'/META506_NC_black_0523.csv','w')\n",
    "    Write = csv.writer(f)\n",
    "    #Write.writerow('label')\n",
    "    \n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        \n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "            cost_ij = train_model(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            #print(cost_ij)\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = [validate_model(i) for i in xrange(n_valid_batches)]\n",
    "                \n",
    "                this_validation_loss = np.mean(validation_losses)\n",
    "               \n",
    "                print \"epoch %i, minibatch %i/%i, validation error %f %%\" % (epoch, minibatch_index + 1, n_train_batches, this_validation_loss * 100)\n",
    "                fp1.write(\"%d\\t%f\\n\" % (epoch, this_validation_loss * 100))\n",
    "\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "                        # 十分改善したならまだ改善の余地があるためpatienceを上げてより多くループを回せるようにする\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                        print \"*** iter %d / patience %d\" % (iter, patience)\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "                    \n",
    "                        \n",
    "                    \n",
    "                    # テストデータのエラー率も計算        \n",
    "                    test_losses = [test_model(i) for i in xrange(n_test_batches)]\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    #print(n_test_batches)\n",
    "                    #print(test_losses)\n",
    "                    test_score = np.mean(test_losses)\n",
    "                    #print(test_score)\n",
    "                    print \"    epoch %i, minibatch %i/%i, test error of best model %f %%\" % (epoch, minibatch_index + 1, n_train_batches, test_score * 100)\n",
    "                    fp2.write(\"%d\\t%f\\n\" % (epoch, test_score * 100))\n",
    "                    \n",
    "            # patienceを超えたらループを終了\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "    \n",
    "    #print value_model(1)\n",
    "        \n",
    "    \n",
    "    fp1.close()\n",
    "    fp2.close()\n",
    "    \n",
    "    \n",
    "    end_time = time.clock()\n",
    "    print \"Optimization complete.\"\n",
    "    listdata = []\n",
    "    testdata = []\n",
    "    for i in range(n_test_batches):                \n",
    "        value=value_model(i)\n",
    "        listdata.append(np.array(value))     \n",
    "        print listdata[0]\n",
    "        for j in range(batch_size):\n",
    "            testdata = listdata[i][j]\n",
    "            Write.writerow([testdata])\n",
    "    f.close()\n",
    "    print \"Best validation score of %f %% obtained at iteration %i, with test performance %f %%\" % (best_validation_loss * 100.0, best_iter + 1, test_score * 100.0)\n",
    "    #print \"The code for file \" + os.path.split(__file__)[1] + \" ran for %.2fm\" % ((end_time - start_time) / 60.0)\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_lenet5(dataset=\"/home/dl-box/Liver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
