{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Graphics Device\n",
      "/home/dl-box/anaconda2/envs/Deep_Test/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from theano.tensor.signal import downsample\n",
    "from theano.tensor.nnet import conv\n",
    "from logistic_sgd2 import LogisticRegression, load_data\n",
    "from mlp import HiddenLayer\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "path = '/home/dl-box/Liver/Cyst_data1016' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('/home/dl-box/Liver/Cyst_data1016/train_dataset1019/3phase_0113_renew.pkl','rb') as f1:\n",
    "    datasets1 = pickle.load(f1)\n",
    "    train_set_x, train_set_y = datasets1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183440\n",
      "(183440, 2523)\n",
      "(183440, 2523)\n"
     ]
    }
   ],
   "source": [
    "print len(train_set_x)\n",
    "train_set_x = np.array(train_set_x)\n",
    "print train_set_x.shape\n",
    "train_set_x = train_set_x.reshape(train_set_x.shape[0],-1)\n",
    "print train_set_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183440, 3, 1, 841)\n",
      "(183440, 2523)\n",
      "183440\n",
      "290696\n",
      "458\n",
      "726\n",
      "epoch 1, minibatch 458/458, validation error 20.315789 %\n",
      "*** iter 457 / patience 20000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "('total size of new array must be unchanged', (400, 841), array([400,   3,  29,  29]))\nApply node that caused the error: GpuReshape{4}(GpuSubtensor{int64:int64:}.0, TensorConstant{[400   3  29  29]})\nInputs types: [CudaNdarrayType(float32, matrix), TensorType(int64, vector)]\nInputs shapes: [(400, 841), (4,)]\nInputs strides: [(841, 1), (8,)]\nInputs values: ['not shown', array([400,   3,  29,  29])]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2ab0f264cfbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m     \u001b[0mevaluate_lenet5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"/home/dl-box/Liver/Cyst_data1016\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-2ab0f264cfbd>\u001b[0m in \u001b[0;36mevaluate_lenet5\u001b[1;34m(learning_rate, n_epochs, dataset, batch_size)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                     \u001b[1;31m# テストデータのエラー率も計算\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m                     \u001b[0mtest_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_test_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dl-box/anaconda2/envs/Deep_Test/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    604\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthunks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mposition_of_error\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m                         storage_map=self.fn.storage_map)\n\u001b[0m\u001b[0;32m    607\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m                     \u001b[1;31m# For the c linker We don't have access from\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dl-box/anaconda2/envs/Deep_Test/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    596\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dl-box/anaconda2/envs/Deep_Test/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    766\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/dl-box/anaconda2/envs/Deep_Test/lib/python2.7/site-packages/theano/sandbox/cuda/basic_ops.pyc\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inp, out_)\u001b[0m\n\u001b[0;32m   2380\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2381\u001b[0m                 raise ValueError(\"total size of new array must be unchanged\",\n\u001b[1;32m-> 2382\u001b[1;33m                                  x.shape, shp)\n\u001b[0m\u001b[0;32m   2383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2384\u001b[0m         \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: ('total size of new array must be unchanged', (400, 841), array([400,   3,  29,  29]))\nApply node that caused the error: GpuReshape{4}(GpuSubtensor{int64:int64:}.0, TensorConstant{[400   3  29  29]})\nInputs types: [CudaNdarrayType(float32, matrix), TensorType(int64, vector)]\nInputs shapes: [(400, 841), (4,)]\nInputs strides: [(841, 1), (8,)]\nInputs values: ['not shown', array([400,   3,  29,  29])]\n\nHINT: Re-running with most Theano optimization disabled could give you a back-trace of when this node was created. This can be done with by setting the Theano flag 'optimizer=fast_compile'. If that does not work, Theano optimizations can be disabled with 'optimizer=None'.\nHINT: Use the Theano flag 'exception_verbosity=high' for a debugprint and storage map footprint of this apply node."
     ]
    }
   ],
   "source": [
    "\n",
    "class LeNetConvPoolLayer(object):\n",
    "    \"\"\"畳み込みニューラルネットの畳み込み層＋プーリング層\"\"\"\n",
    "    def __init__(self, rng, input, image_shape, filter_shape, poolsize=(2, 2)):\n",
    "        # 入力の特徴マップ数は一致する必要がある\n",
    "        assert image_shape[1] == filter_shape[1]\n",
    "\n",
    "        fan_in = np.prod(filter_shape[1:])\n",
    "        fan_out = filter_shape[0] * np.prod(filter_shape[2:]) / np.prod(poolsize)\n",
    "\n",
    "        W_bound = np.sqrt(6.0 / (fan_in + fan_out))#重み付け層の前後の隠れ層の数　重みの初期値を定義ー論文:Glorot & Bengio (2010)\n",
    "        self.W = theano.shared(\n",
    "            np.asarray(rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "                       dtype=theano.config.floatX),  # @UndefinedVariable\n",
    "            borrow=True)\n",
    "\n",
    "        b_values = np.zeros((filter_shape[0],), dtype=theano.config.floatX)  # @UndefinedVariable\n",
    "        self.b = theano.shared(value=b_values, borrow=T)#フィルター=W，バイアス=b\n",
    "\n",
    "        # 入力の特徴マップとフィルタの畳み込み\n",
    "        conv_out = conv.conv2d(\n",
    "            input=input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            image_shape=image_shape)#filter_shape:カーネルサイズ,image_filter:最初のサイズ\n",
    "\n",
    "        # Max-poolingを用いて各特徴マップをダウンサンプリング\n",
    "        pooled_out = downsample.max_pool_2d(\n",
    "            input=conv_out,\n",
    "            ds=poolsize,\n",
    "            ignore_border=True)\n",
    "\n",
    "        # バイアスを加える\n",
    "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))#ここではtanh()で処理している\n",
    "\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "\n",
    "def evaluate_lenet5(learning_rate=0.0005, n_epochs=10,\n",
    "                            dataset=\"/home/dl-box/Liver/Cyst_data1016\", batch_size=400):\n",
    "    rng = np.random.RandomState(23455)\n",
    "\n",
    "\n",
    "            # 学習データのロード\n",
    "            #datasets = load_data(dataset)\n",
    "    with open('/home/dl-box/Liver/Cyst_data1016/train_dataset1019/3phase_0113_renew.pkl','rb') as f1:\n",
    "                datasets1 = pickle.load(f1)\n",
    "            \n",
    "    with open('/home/dl-box/Liver/Cyst_data1016/test_dataset1019/test_126_ART.pkl','rb') as f2:\n",
    "                datasets2 = pickle.load(f2)\n",
    "    \n",
    "    train_set_x, train_set_y = datasets1[0]\n",
    "    valid_set_x, valid_set_y = datasets1[1]\n",
    "    test_set_x, test_set_y = datasets2[0]\n",
    "    \n",
    "    train_set_x = np.array(train_set_x)\n",
    "    train_set_y = np.array(train_set_y)    \n",
    "    valid_set_x = np.array(valid_set_x)\n",
    "    valid_set_y = np.array(valid_set_y)  \n",
    "    test_set_x = np.array(test_set_x)\n",
    "    test_set_y = np.array(test_set_y)  \n",
    "    \n",
    "    print train_set_x.shape\n",
    "    \n",
    "    train_set_x = train_set_x.reshape(train_set_x.shape[0],-1)\n",
    "    train_set_y = train_set_y.reshape(train_set_y.shape[0],-1)\n",
    "    valid_set_x = valid_set_x.reshape(valid_set_x.shape[0],-1)\n",
    "    valid_set_y = valid_set_y.reshape(valid_set_y.shape[0],-1)\n",
    "    test_set_x = test_set_x.reshape(test_set_x.shape[0],-1)\n",
    "    test_set_y = test_set_y.reshape(test_set_y.shape[0],-1)\n",
    "    \n",
    "    print train_set_x.shape\n",
    "    \n",
    "    train_dimension = train_set_x.shape[0]\n",
    "    valid_dimension = valid_set_x.shape[0]\n",
    "    test_dimension = test_set_x.shape[0]\n",
    "    \n",
    "    print train_dimension\n",
    "    print test_dimension\n",
    "\n",
    "    n_train_batches = train_dimension / batch_size\n",
    " \n",
    "    n_valid_batches = valid_dimension / batch_size\n",
    "    \n",
    "    n_test_batches = test_dimension / batch_size\n",
    "    \n",
    "    print n_train_batches\n",
    "    print n_test_batches\n",
    "\n",
    "   \n",
    "    train_set_x = theano.shared(np.asarray(train_set_x,dtype=theano.config.floatX),borrow=True)\n",
    "    train_set_y = theano.shared(np.asarray(train_set_y,dtype=theano.config.floatX),borrow=True)   \n",
    "    valid_set_x = theano.shared(np.asarray(valid_set_x,dtype=theano.config.floatX),borrow=True)\n",
    "    valid_set_y = theano.shared(np.asarray(valid_set_y,dtype=theano.config.floatX),borrow=True)\n",
    "    test_set_x = theano.shared(np.asarray(test_set_x,dtype=theano.config.floatX),borrow=True)\n",
    "    test_set_y = theano.shared(np.asarray(test_set_y,dtype=theano.config.floatX),borrow=True)\n",
    "    \n",
    "            # ミニバッチのインデックスを表すシンボル\n",
    "    \n",
    "    index = T.lscalar() #←change\n",
    " \n",
    "            # ミニバッチの学習データ(ピクセル)とラベル(0－9)を表すシンボル\n",
    "    \n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y')\n",
    "    \n",
    "    # 入力\n",
    "    # 入力のサイズを4Dテンソルに変換\n",
    "    # batch_sizeは訓練画像の枚数\n",
    "    # チャンネル数は1\n",
    "    # (28, 28)はMNISTの画像サイズ\n",
    "    layer0_input = x.reshape((batch_size, 3, 29, 29))   #←エラー起きてる？\n",
    " \n",
    "    # 最初の畳み込み層+プーリング層\n",
    "    # 畳み込みに使用するフィルタサイズは5x5ピクセル\n",
    "    # 畳み込みによって画像サイズは28x28ピクセルから24x24ピクセルに落ちる\n",
    "    # プーリングによって画像サイズはさらに12x12ピクセルに落ちる\n",
    "    # 特徴マップ数は20枚でそれぞれの特徴マップのサイズは12x12ピクセル\n",
    "    # 最終的にこの層の出力のサイズは (batch_size, 20, 12, 12) になる\n",
    "   \n",
    "    layer0 = LeNetConvPoolLayer(rng,\n",
    "                input=layer0_input,\n",
    "                image_shape=(batch_size, 3, 29, 29),  # 入力画像のサイズを4Dテンソルで指定\n",
    "                filter_shape=(20, 3, 6, 6),           # フィルタのサイズを4Dテンソルで指定\n",
    "                poolsize=(2, 2))\n",
    "    \n",
    "    #print layer0_input\n",
    "    # layer0の出力がlayer1への入力となる\n",
    "    # layer0の出力画像のサイズは (batch_size, 20, 12, 12)\n",
    "    # 12x12ピクセルの画像が特徴マップ数分（20枚）ある\n",
    "    # 畳み込みによって画像サイズは12x12ピクセルから8x8ピクセルに落ちる\n",
    "    # プーリングによって画像サイズはさらに4x4ピクセルに落ちる\n",
    "    # 特徴マップ数は50枚でそれぞれの特徴マップのサイズは4x4ピクセル\n",
    "    # 最終的にこの層の出力のサイズは (batch_size, 50, 4, 4) になる\n",
    "   \n",
    "    layer1 = LeNetConvPoolLayer(rng,\n",
    "                input=layer0.output,\n",
    "                image_shape=(batch_size, 20, 12, 12), # 入力画像のサイズを4Dテンソルで指定\n",
    "                filter_shape=(50, 20, 5, 5),          # フィルタのサイズを4Dテンソルで指定\n",
    "                poolsize=(2, 2))\n",
    "\n",
    "    # 隠れ層への入力\n",
    "    # 画像のピクセルをフラット化する\n",
    "    # layer1の出力のサイズは (batch_size, 50, 4, 4) なのでflatten()によって\n",
    "    # (batch_size, 50*4*4) = (batch_size, 800) になる\n",
    "    layer2_input = layer1.output.flatten(2)\n",
    "\n",
    "    # 全結合された隠れ層\n",
    "    # 入力が800ユニット、出力が500ユニット\n",
    "  \n",
    "    layer2 = HiddenLayer(rng,\n",
    "        input=layer2_input,\n",
    "        n_in=50 * 4 * 4,\n",
    "        n_out=300,\n",
    "        activation=T.tanh)\n",
    "\n",
    "    # 最終的な数字分類を行うsoftmax層\n",
    "    layer3 = LogisticRegression(input=layer2.output, n_in=300, n_out=2)\n",
    "\n",
    "  \n",
    "    # コスト関数を計算するシンボル\n",
    "    cost = layer3.negative_log_likelihood(y)\n",
    "\n",
    "    #test_x = T.lvector(test_set_x)\n",
    "    #print(test_x[0:100])\n",
    "    #test_y = T.lvector(test_set_y)\n",
    "    #test_set_x = test_set_x.flatten()\n",
    "    \n",
    "    test_set_y = test_set_y.flatten()#change from matrix to vector \n",
    "    \n",
    "  \n",
    "    \n",
    "    #test_x = T.cast(test_x,'int32')\n",
    "    \n",
    "    test_set_y = T.cast(test_set_y,'int32')#cast from float32 to int32\n",
    " \n",
    "    \n",
    "    value = layer3.makefile1(y)\n",
    "    \n",
    "    \n",
    "    # index番目のテスト用ミニバッチを入力してエラー率を返す関数を定義\n",
    "    test_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        })\n",
    "    \n",
    "    value_model = theano.function(\n",
    "        [index],\n",
    "        value,\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        })\n",
    "\n",
    " \n",
    "    valid_set_y = valid_set_y.flatten()#change from matrix to vector \n",
    "    \n",
    "\n",
    "    valid_set_y = T.cast(valid_set_y,'int32')#cast from float32 to int32\n",
    "  \n",
    "    # index番目のバリデーション用ミニバッチを入力してエラー率を返す関数を定義\n",
    "    \n",
    "    \n",
    "    validate_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        })\n",
    "\n",
    "    # パラメータ\n",
    "    params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "    #print(params)\n",
    "\n",
    "    # コスト関数の微分\n",
    "    grads = T.grad(cost, params)\n",
    "\n",
    "    # パラメータ更新式\n",
    "    updates = [(param_i, param_i - learning_rate * grad_i) for param_i, grad_i in zip(params, grads)]\n",
    "    #print(updates)\n",
    "    \n",
    "   \n",
    "    train_set_y = train_set_y.flatten()#change from matrix to vector \n",
    "    \n",
    "\n",
    "    train_set_y = T.cast(train_set_y,'int32')#cast from float32 to int32\n",
    " \n",
    "    # index番目の訓練バッチを入力し、パラメータを更新する関数を定義\n",
    "    \n",
    "    train_model = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        })\n",
    "\n",
    "\n",
    "    # eary-stoppingのパラメータ\n",
    "    patience = 20000\n",
    "    patience_increase = 2\n",
    "    improvement_threshold = 0.95\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "  \n",
    "\n",
    "    best_validation_loss = np.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0\n",
    "    start_time = time.clock()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "\n",
    "    fp1 = open(\"validation_error.txt\", \"w\")\n",
    "    fp2 = open(\"test_error.txt\", \"w\")\n",
    "    f = open(path+'/output0113_3phase_126_ART.csv','w')\n",
    "    Write = csv.writer(f)\n",
    "    #Write.writerow('label')\n",
    "    \n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        \n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "            cost_ij = train_model(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "            #print(cost_ij)\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = [validate_model(i) for i in xrange(n_valid_batches)]\n",
    "                \n",
    "                this_validation_loss = np.mean(validation_losses)\n",
    "               \n",
    "                print \"epoch %i, minibatch %i/%i, validation error %f %%\" % (epoch, minibatch_index + 1, n_train_batches, this_validation_loss * 100)\n",
    "                fp1.write(\"%d\\t%f\\n\" % (epoch, this_validation_loss * 100))\n",
    "\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "                        # 十分改善したならまだ改善の余地があるためpatienceを上げてより多くループを回せるようにする\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                        print \"*** iter %d / patience %d\" % (iter, patience)\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "                    \n",
    "                        \n",
    "                    \n",
    "                    # テストデータのエラー率も計算        \n",
    "                    test_losses = [test_model(i) for i in xrange(n_test_batches)]\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    #print(n_test_batches)\n",
    "                    #print(test_losses)\n",
    "                    test_score = np.mean(test_losses)\n",
    "                    #print(test_score)\n",
    "                    print \"    epoch %i, minibatch %i/%i, test error of best model %f %%\" % (epoch, minibatch_index + 1, n_train_batches, test_score * 100)\n",
    "                    fp2.write(\"%d\\t%f\\n\" % (epoch, test_score * 100))\n",
    "                    \n",
    "            # patienceを超えたらループを終了\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "    \n",
    "    fp1.close()\n",
    "    fp2.close()\n",
    "    \n",
    "    \n",
    "    end_time = time.clock()\n",
    "    print \"Optimization complete.\"\n",
    "    listdata = []\n",
    "    testdata = []\n",
    "    for i in range(n_test_batches):                \n",
    "        value=value_model(i)\n",
    "        listdata.append(np.array(value))                \n",
    "        for j in range(batch_size):\n",
    "            testdata = listdata[i][j]\n",
    "            Write.writerow([testdata])\n",
    "    f.close()\n",
    "    print \"Best validation score of %f %% obtained at iteration %i, with test performance %f %%\" % (best_validation_loss * 100.0, best_iter + 1, test_score * 100.0)\n",
    "    #print \"The code for file \" + os.path.split(__file__)[1] + \" ran for %.2fm\" % ((end_time - start_time) / 60.0)\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_lenet5(dataset=\"/home/dl-box/Liver/Cyst_data1016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
