{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from theano.tensor.signal import downsample\n",
    "from theano.tensor.nnet import conv\n",
    "from logistic_sgd2 import LogisticRegression, load_data\n",
    "from mlp import HiddenLayer\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "with open('.pkl','rb') as f1:\n",
    "    datasets1 = pickle.load(f1)\n",
    "    train_set_x, train_set_y = datasets1[0]\n",
    "    \n",
    "train_set_x = np.array(train_set_x)\n",
    "train_set_x = train_set_x.reshape(train_set_x.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class LeNetConvPoolLayer(object):\n",
    "    \"\"\"CNNのstructure\"\"\"\n",
    "    def __init__(self, rng, input, image_shape, filter_shape, poolsize=(2, 2)):\n",
    "        assert image_shape[1] == filter_shape[1]\n",
    "\n",
    "        fan_in = np.prod(filter_shape[1:])\n",
    "        fan_out = filter_shape[0] * np.prod(filter_shape[2:]) / np.prod(poolsize)\n",
    "\n",
    "        W_bound = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "        self.W = theano.shared(\n",
    "            np.asarray(rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "                       dtype=theano.config.floatX),\n",
    "            borrow=True)\n",
    "\n",
    "        b_values = np.zeros((filter_shape[0],), dtype=theano.config.floatX)  \n",
    "        self.b = theano.shared(value=b_values, borrow=T)\n",
    "\n",
    "        # 特徴マップ&フィルタの畳み込み\n",
    "        conv_out = conv.conv2d(\n",
    "            input=input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            image_shape=image_shape)#filter_shape:カーネルサイズ,image_filter:最初のサイズ\n",
    "\n",
    "        # Max-poolingでのダウンサンプリング\n",
    "        pooled_out = downsample.max_pool_2d(\n",
    "            input=conv_out,\n",
    "            ds=poolsize,\n",
    "            ignore_border=True)\n",
    "\n",
    "        # バイアスを加える\n",
    "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))#ここではtanh()で処理している\n",
    "\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "\n",
    "def evaluate_lenet5(learning_rate=0.0005, n_epochs=100,\n",
    "                            dataset=\"/\", batch_size=400):\n",
    "    rng = np.random.RandomState(23455)\n",
    "\n",
    "\n",
    "            # 学習データのロード\n",
    "            #datasets = load_data(dataset)\n",
    "    with open('.pkl','rb') as f1:\n",
    "                datasets1 = pickle.load(f1)\n",
    "            \n",
    "    with open('.pkl','rb') as f2:\n",
    "                datasets2 = pickle.load(f2)\n",
    "    \n",
    "    train_set_x, train_set_y = datasets1[0]\n",
    "    valid_set_x, valid_set_y = datasets1[1]\n",
    "    test_set_x, test_set_y = datasets2[0]\n",
    "    \n",
    "    train_set_x = np.array(train_set_x)\n",
    "    train_set_y = np.array(train_set_y)    \n",
    "    valid_set_x = np.array(valid_set_x)\n",
    "    valid_set_y = np.array(valid_set_y)  \n",
    "    test_set_x = np.array(test_set_x)\n",
    "    test_set_y = np.array(test_set_y)  \n",
    "\n",
    "    \n",
    "    train_set_x = train_set_x.reshape(train_set_x.shape[0],-1)\n",
    "    train_set_y = train_set_y.reshape(train_set_y.shape[0],-1)\n",
    "    valid_set_x = valid_set_x.reshape(valid_set_x.shape[0],-1)\n",
    "    valid_set_y = valid_set_y.reshape(valid_set_y.shape[0],-1)\n",
    "    test_set_x = test_set_x.reshape(test_set_x.shape[0],-1)\n",
    "    test_set_y = test_set_y.reshape(test_set_y.shape[0],-1)\n",
    "    \n",
    "    \n",
    "    train_dimension = train_set_x.shape[0]\n",
    "    valid_dimension = valid_set_x.shape[0]\n",
    "    test_dimension = test_set_x.shape[0]\n",
    "\n",
    "\n",
    "    n_train_batches = train_dimension / batch_size\n",
    "    n_valid_batches = valid_dimension / batch_size   \n",
    "    n_test_batches = test_dimension / batch_size\n",
    "   \n",
    "    train_set_x = theano.shared(np.asarray(train_set_x,dtype=theano.config.floatX),borrow=True)\n",
    "    train_set_y = theano.shared(np.asarray(train_set_y,dtype=theano.config.floatX),borrow=True)   \n",
    "    valid_set_x = theano.shared(np.asarray(valid_set_x,dtype=theano.config.floatX),borrow=True)\n",
    "    valid_set_y = theano.shared(np.asarray(valid_set_y,dtype=theano.config.floatX),borrow=True)\n",
    "    test_set_x = theano.shared(np.asarray(test_set_x,dtype=theano.config.floatX),borrow=True)\n",
    "    test_set_y = theano.shared(np.asarray(test_set_y,dtype=theano.config.floatX),borrow=True)\n",
    "    \n",
    "    index = T.lscalar() #←change\n",
    "\n",
    "    \n",
    "    x = T.matrix('x')\n",
    "    y = T.ivector('y')\n",
    "    \n",
    "    layer0_input = x.reshape((batch_size, 1, 29, 29))   #←input\n",
    "   \n",
    "    layer0 = LeNetConvPoolLayer(rng,\n",
    "                input=layer0_input,\n",
    "                image_shape=(batch_size, 3, 29, 29),  \n",
    "                filter_shape=(20, 3, 6, 6), \n",
    "                poolsize=(2, 2))\n",
    "\n",
    "    layer1 = LeNetConvPoolLayer(rng,\n",
    "                input=layer0.output,\n",
    "                image_shape=(batch_size, 20, 12, 12), \n",
    "                filter_shape=(50, 20, 5, 5),        \n",
    "                poolsize=(2, 2))\n",
    "\n",
    "    layer2_input = layer1.output.flatten(2)\n",
    "  \n",
    "    layer2 = HiddenLayer(rng,\n",
    "        input=layer2_input,\n",
    "        n_in=50 * 4 * 4,\n",
    "        n_out=300,\n",
    "        activation=T.tanh)\n",
    "\n",
    "    # softmax処理\n",
    "    layer3 = LogisticRegression(input=layer2.output, n_in=300, n_out=2)\n",
    "\n",
    "  \n",
    "    # コスト関数を計算するシンボル\n",
    "    cost = layer3.negative_log_likelihood(y)\n",
    "\n",
    "    test_set_y = test_set_y.flatten()#change from matrix to vector\n",
    "    test_set_y = T.cast(test_set_y,'int32')#cast from float32 to int32\n",
    "    value = layer3.makefile1(y)\n",
    "    \n",
    "    #calcurate the error rate \n",
    "    test_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        })\n",
    "    \n",
    "    value_model = theano.function(\n",
    "        [index],\n",
    "        value,\n",
    "        givens={\n",
    "            x: test_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: test_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        })\n",
    "\n",
    " \n",
    "    valid_set_y = valid_set_y.flatten()#change from matrix to vector \n",
    "    valid_set_y = T.cast(valid_set_y,'int32')#cast from float32 to int32\n",
    "    validate_model = theano.function(\n",
    "        [index],\n",
    "        layer3.errors(y),\n",
    "        givens={\n",
    "            x: valid_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: valid_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        })\n",
    "\n",
    "    # パラメータ\n",
    "    params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "    grads = T.grad(cost, params)\n",
    "\n",
    "    updates = [(param_i, param_i - learning_rate * grad_i) for param_i, grad_i in zip(params, grads)]\n",
    "    train_set_y = train_set_y.flatten()#change from matrix to vector \n",
    "    train_set_y = T.cast(train_set_y,'int32')#cast from float32 to int32\n",
    "     \n",
    "    train_model = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "        })\n",
    "\n",
    "\n",
    "    # eary-stoppingのパラメータ\n",
    "    patience = 10000\n",
    "    patience_increase = 2\n",
    "    improvement_threshold = 0.95\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "  \n",
    "\n",
    "    best_validation_loss = np.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0\n",
    "    start_time = time.clock()\n",
    "\n",
    "    epoch = 0\n",
    "    done_looping = False\n",
    "    f = open(path+'.csv','w')\n",
    "    Write = csv.writer(f)  \n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        \n",
    "        for minibatch_index in xrange(n_train_batches):\n",
    "            cost_ij = train_model(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "                validation_losses = [validate_model(i) for i in xrange(n_valid_batches)]   \n",
    "                this_validation_loss = np.mean(validation_losses)  \n",
    "                print \"epoch %i, minibatch %i/%i, validation error %f %%\" % (epoch, minibatch_index + 1, n_train_batches, this_validation_loss * 100)\n",
    "                fp1.write(\"%d\\t%f\\n\" % (epoch, this_validation_loss * 100))\n",
    "\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "                    if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                        print \"*** iter %d / patience %d\" % (iter, patience)\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "                    \n",
    "                        \n",
    "                    \n",
    "                    # テストエラー率計算        \n",
    "                    test_losses = [test_model(i) for i in xrange(n_test_batches)]\n",
    "                    \n",
    "                    test_score = np.mean(test_losses)\n",
    "                    print \"    epoch %i, minibatch %i/%i, test error of best model %f %%\" % (epoch, minibatch_index + 1, n_train_batches, test_score * 100)\n",
    "                    fp2.write(\"%d\\t%f\\n\" % (epoch, test_score * 100))\n",
    "                    \n",
    "            # patienceを超えたらループを終了\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "    \n",
    "    fp1.close()\n",
    "    fp2.close()\n",
    "    \n",
    "    \n",
    "    end_time = time.clock()\n",
    "    print \"Optimization complete.\"\n",
    "    listdata = []\n",
    "    testdata = []\n",
    "    for i in range(n_test_batches):                \n",
    "        value=value_model(i)\n",
    "        listdata.append(np.array(value))                \n",
    "        for j in range(batch_size):\n",
    "            testdata = listdata[i][j]\n",
    "            Write.writerow([testdata])\n",
    "    f.close()\n",
    "    print \"Best validation score of %f %% obtained at iteration %i, with test performance %f %%\" % (best_validation_loss * 100.0, best_iter + 1, test_score * 100.0)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    evaluate_lenet5(dataset=\"/home/dl-box/Liver/Cyst_data1016\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
